# ğŸ¤– Multi-Agent Coding Assistant

A production-quality multi-agent pipeline that autonomously writes, reviews, tests, and optimizes Python code â€” built with LangGraph, Claude AI, RAG, MCP, and LangSmith.

---

## ğŸ† Benchmark Results

| Difficulty | Tasks | Avg Score | Tests Passed |
|-----------|-------|-----------|-------------|
| Easy      | 4     | **100%**  | 4/4 âœ…       |
| Medium    | 4     | **85%**   | 4/4 âœ…       |
| Hard      | 2     | **80%**   | 2/2 âœ…       |
| **Overall** | **10** | **88%** | **10/10 âœ…** |

> DSPy prompt optimization confirmed pipeline baseline at **88%** â€” near-optimal for task set.

---

## ğŸ—ï¸ Architecture

```
USER TASK
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangGraph Pipeline                  â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Planner  â”‚â”€â”€â”€â–¶â”‚ Code Writer â”‚â—€â”€â”€â”€ Qdrant RAG â”‚
â”‚  â”‚  Agent   â”‚    â”‚    Agent    â”‚    (TF-IDF,    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   15 docs)     â”‚
â”‚                         â”‚                        â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                  â”‚  Reviewer   â”‚                 â”‚
â”‚                  â”‚    Agent    â”‚                 â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                         â”‚                        â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                  â”‚ Test Writer â”‚â—€â”€â”€â”€ Docker      â”‚
â”‚                  â”‚    Agent    â”‚     Sandbox     â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚   Final    â”‚
                    â”‚   Output   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                   LangSmith Traces
                   (full observability)
```

---

## âœ¨ Features

- **Multi-Agent Orchestration** â€” LangGraph state machine with 4 specialized agents: Planner, Code Writer, Reviewer, Test Writer
- **RAG Knowledge Retrieval** â€” Qdrant vector database with TF-IDF indexing over 15 Python best-practice documents; queried before every code generation step
- **Sandboxed Code Execution** â€” pytest runs in an isolated Docker container; zero risk to host machine
- **MCP Tool Server** â€” 6 tools exposed via Model Context Protocol: `write_file`, `read_file`, `execute_code`, `search_web`, `query_docs`, `list_files`
- **Automated Evaluation** â€” 5-dimension scoring system (syntax, type hints, docstrings, error handling, test pass rate) across 10 benchmark tasks
- **LangSmith Observability** â€” Full tracing of every LLM call, token count, latency, and agent chain
- **DSPy Optimization** â€” BootstrapFewShot prompt optimizer integrated; baseline confirmed at 88%
- **Iterative Refinement** â€” Code Writer loops up to 3 times based on Reviewer feedback before finalizing

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Agent Orchestration | LangGraph |
| LLM | Claude Opus 4.5 (Anthropic) |
| RAG Vector DB | Qdrant (in-memory/persistent) |
| RAG Embeddings | TF-IDF (pure Python, zero DLL deps) |
| Tool Protocol | MCP (Model Context Protocol) |
| Code Execution | Docker (sandboxed pytest) |
| Observability | LangSmith |
| Prompt Optimization | DSPy (BootstrapFewShot) |
| Web Search | Tavily API |

---

## ğŸ“ Project Structure

```
multi-agent-coding-assistant/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ graph.py          # LangGraph state machine + run_pipeline()
â”‚   â”œâ”€â”€ planner.py        # Task decomposition agent
â”‚   â”œâ”€â”€ code_writer.py    # Code generation agent (RAG-enhanced)
â”‚   â”œâ”€â”€ reviewer.py       # Code quality reviewer
â”‚   â””â”€â”€ test_writer.py    # pytest generation + Docker execution
â”œâ”€â”€ mcp_server/
â”‚   â””â”€â”€ server.py         # MCP server with 6 tools
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ indexer.py        # TF-IDF vocabulary builder + Qdrant upsert
â”‚   â”œâ”€â”€ retriever.py      # Query engine (query_points API)
â”‚   â””â”€â”€ vocab.json        # Saved vocabulary for query-time vectorization
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ metrics.py        # 5-dimension code quality scorer
â”‚   â”œâ”€â”€ benchmarks.py     # 10 benchmark tasks (easy/medium/hard)
â”‚   â””â”€â”€ eval_results.json # Latest benchmark results
â”œâ”€â”€ dspy_optimizer/
â”‚   â”œâ”€â”€ signatures.py     # DSPy signatures for each agent
â”‚   â”œâ”€â”€ optimizer.py      # BootstrapFewShot optimization loop
â”‚   â””â”€â”€ optimized_pipeline.json
â”œâ”€â”€ langsmith_dataset.py  # Pushes benchmarks to LangSmith datasets
â”œâ”€â”€ langsmith_setup.py    # LangSmith connection verification
â”œâ”€â”€ run_eval.py           # Evaluation runner
â”œâ”€â”€ smoke_test.py         # Quick end-to-end test
â””â”€â”€ .env                  # API keys (not committed)
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Docker Desktop (for sandboxed test execution)
- API keys: Anthropic, Tavily, LangSmith

### Installation

```bash
git clone https://github.com/yourusername/multi-agent-coding-assistant
cd multi-agent-coding-assistant

pip install langgraph langchain-anthropic mcp tavily-python \
            qdrant-client langsmith dspy-ai python-dotenv
```

### Configure environment

```bash
cp .env.example .env
# Fill in your API keys:
# ANTHROPIC_API_KEY=sk-ant-...
# TAVILY_API_KEY=tvly-...
# LANGCHAIN_API_KEY=lsv2_pt_...
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_PROJECT=multi-agent-coder
```

### Build the RAG index

```bash
python rag/indexer.py
```

### Run a quick smoke test

```bash
python smoke_test.py
```

### Run full evaluation

```bash
python run_eval.py   # edit difficulty="easy/medium/hard/all"
```

### Run DSPy optimization

```bash
python dspy_optimizer/optimizer.py
```

---

## ğŸ“Š Evaluation Scoring

Each generated function is scored across 5 dimensions:

| Metric | Weight | Detection Method |
|--------|--------|-----------------|
| Syntax Valid | 25% | `ast.parse()` |
| Type Hints | 20% | Regex: `-> type` + `param: type` |
| Docstring | 20% | Triple-quote string present |
| Error Handling | 20% | `raise` or `try/except` block |
| Test Pass Rate | 15% | pytest exit code in Docker |

---

## ğŸ” LangSmith Tracing

Every pipeline run is fully traced in LangSmith:

```
benchmark-run (task_name)
  â””â”€â”€ multi-agent-pipeline
        â”œâ”€â”€ planner-agent        â†’ plan + LLM tokens
        â”œâ”€â”€ code-writer-agent    â†’ RAG query + generated code
        â”œâ”€â”€ reviewer-agent       â†’ score + feedback
        â””â”€â”€ test-writer-agent    â†’ pytest result
```

View live traces at [smith.langchain.com](https://smith.langchain.com) after running any eval.

---

## ğŸ’¡ Key Engineering Decisions

**Why Qdrant over ChromaDB?**
ChromaDB's sentence-transformers dependency pulls in PyTorch, causing DLL failures on Windows. Qdrant with pure-Python TF-IDF has zero native dependencies and works reliably cross-platform.

**Why TF-IDF over neural embeddings?**
For a knowledge base of 15 curated documents, TF-IDF retrieval is fast, deterministic, and requires no GPU. Neural embeddings add latency and dependency complexity without meaningful accuracy gains at this scale.

**Why MCP for tooling?**
MCP provides a standardized protocol for tool exposure, making it easy to add/swap tools without touching agent logic. The same tool server can serve multiple agents or be replaced with a remote server.

**Why Docker for test execution?**
Generated code is untrusted â€” running it directly on the host machine is a security risk. Docker provides complete isolation with a predictable Python + pytest environment.

---

## ğŸ“ˆ Sample Output

```
[1/4] Running: binary_search (medium)...

  âœ…  Syntax Valid         100%
  âœ…  Type Hints           100%
  âœ…  Docstring            100%
  âœ…  Error Handling       100%
  âœ…  Tests Passed         100%

  TOTAL SCORE: 100.0 / 100   (98.4s)
  Iterations: 1
```

---

## ğŸ—“ï¸ Build Timeline

| Day | Focus | Key Deliverable |
|-----|-------|----------------|
| Day 1 | Foundation | LangGraph pipeline + 4 agents + MCP server |
| Day 2 | Tools & Testing | Docker sandbox + Tavily search + smoke tests |
| Day 3 | RAG + Evaluation | Qdrant RAG + 10-task benchmark + LangSmith |
| Day 4 | Polish | DSPy optimization + README + portfolio ready |

---

## ğŸ§‘â€ğŸ’» Author

**Mani Sharma**
Built as a portfolio project demonstrating multi-agent AI systems, LLM orchestration, RAG pipelines, and production-quality evaluation frameworks.

---

## ğŸ“„ License

MIT License â€” free to use, modify, and distribute.
